{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意到 `dataset.py` 中直接从 json 中读取了数据，但数据集并不符合这种格式，所以需要将数据集转换为 json 格式。\n",
    "\n",
    "```py\n",
    "# 复现代码中读取数据集的部分\n",
    "            all_raw_base, all_raw_a, all_raw_b, all_raw_res = json.load(open('%s/raw_data'%(total_raw_data_path)))\n",
    "```\n",
    "\n",
    "所以我们需要将数据集转换为 json 格式：\n",
    "\n",
    "```\n",
    "[\n",
    "    [\"base1\", \"base2\", \"base3\"],\n",
    "    [\"a1\", \"a2\", \"a3\"],\n",
    "    [\"b1\", \"b2\", \"b3\"],\n",
    "    [\"res1\", \"res2\", \"res3\"]\n",
    "]\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48785/48785 [00:18<00:00, 2667.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151426\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'RAW_DATA/fse2022'\n",
    "out_file = 'RAW_DATA/raw_data'\n",
    "# walk 递归找到目录下的所有 json 文件\n",
    "import os\n",
    "\n",
    "def get_all_json_files(path):\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            if file.endswith('metadata.json'):\n",
    "                yield os.path.join(root, file)\n",
    "\n",
    "all_json_files = list(get_all_json_files(data_dir))\n",
    "print(len(all_json_files))\n",
    "\n",
    "# 读取 json 文件\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "o_contents = []\n",
    "a_contents = []\n",
    "b_contents = []\n",
    "r_contents = []\n",
    "for file in tqdm(all_json_files):\n",
    "    with open(file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        for chunk in data['conflicting_chunks']:\n",
    "            if chunk['res_region'] is None:\n",
    "                continue\n",
    "            o_contents.append(chunk['base_contents'])\n",
    "            a_contents.append(chunk['a_contents'])\n",
    "            b_contents.append(chunk['b_contents'])\n",
    "            r_contents.append(chunk['res_region'])\n",
    "    \n",
    "assert len(o_contents) == len(a_contents) == len(b_contents) == len(r_contents)\n",
    "print(len(o_contents))\n",
    "\n",
    "json_arr = [\n",
    "    o_contents,\n",
    "    a_contents,\n",
    "    b_contents,\n",
    "    r_contents\n",
    "]\n",
    "\n",
    "# 把 json_arr 写入文件\n",
    "with open(out_file, 'w') as f:\n",
    "    json.dump(json_arr, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自己收集的数据集 .json 转化为 raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "358446\n",
      "358446\n",
      "358446\n",
      "358446\n"
     ]
    }
   ],
   "source": [
    "data_dir = '/root/projects/conflictManager/edit_script_resolver/train_and_infer/data/processed_data/recollect_without_min_bundle_without_file_content'\n",
    "out_file = 'RAW_DATA/graphQL_raw_data_sample_20'\n",
    "\n",
    "\n",
    "# 1. 列出 data_dir 下所有 xx.json 文件\n",
    "def get_all_json_files(path):\n",
    "    import os\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            if file.endswith('.json'):\n",
    "                from pathlib import Path\n",
    "                basename = Path(file).stem\n",
    "                idx = basename.split('.')[0]\n",
    "                yield (os.path.join(root, file), idx)\n",
    "\n",
    "tuples = tuple(get_all_json_files(data_dir))\n",
    "\n",
    "o_contents = []\n",
    "a_contents = []\n",
    "b_contents = []\n",
    "r_contents = []\n",
    "\n",
    "\n",
    "# 2. 读取 json 文件\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "for file, idx in tqdm(tuples, dynamic_ncols=True, desc='Reading json files', leave=False, position=0):\n",
    "    if (int(idx) >= 20): continue\n",
    "    with open(file, 'r') as f:\n",
    "        cfs = json.load(f)\n",
    "        for cf in tqdm(cfs, dynamic_ncols=True, desc='Reading conflict chunks', leave=False, position=1):\n",
    "            for chunk in cf['conflict_chunks']:\n",
    "                o_contents.append(chunk['o_content'])\n",
    "                a_contents.append(chunk['a_content'])\n",
    "                b_contents.append(chunk['b_content'])\n",
    "                r_contents.append(chunk['r_content'])\n",
    "    assert len(o_contents) == len(a_contents) == len(b_contents) == len(r_contents)\n",
    "\n",
    "print(len(o_contents))\n",
    "print(len(a_contents))\n",
    "print(len(b_contents))\n",
    "print(len(r_contents))\n",
    "\n",
    "json_arr = [\n",
    "    o_contents,\n",
    "    a_contents,\n",
    "    b_contents,\n",
    "    r_contents\n",
    "]\n",
    "\n",
    "# 把 json_arr 写入文件\n",
    "with open(out_file, 'w') as f:\n",
    "    json.dump(json_arr, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 看看 token_len 分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 151426/151426 [04:03<00:00, 620.89it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {True: 138641, False: 12785})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 找到所有符合这个模式的文件\n",
    "\n",
    "# data_path = 'RAW_DATA/raw_data'\n",
    "data_path = 'RAW_DATA/graphQL_raw_data_sample_20'\n",
    "\n",
    "# 内容是 all_raw_base, all_raw_a, all_raw_b, all_raw_res = json.load(open(data_path, 'r'))\n",
    "# 统计所有 inputs 和 outputs 的长度分布\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "from tqdm import tqdm   \n",
    "from collections import defaultdict\n",
    "from transformers import RobertaTokenizer, T5Model, T5ForConditionalGeneration, AdamW\n",
    "\n",
    "# 模型类型设定为 CodeT5 的小模型\n",
    "model_type = 'Salesforce/codet5-small'\n",
    "local_path = './codet5/codet5-small'\n",
    "\n",
    "# 初始化对应的分词器\n",
    "# tokenizer = RobertaTokenizer.from_pretrained(model_type)\n",
    "tokenizer = RobertaTokenizer.from_pretrained(local_path)\n",
    "\n",
    "\n",
    "# inputs_lens = defaultdict(int)\n",
    "# outputs_lens = defaultdict(int)\n",
    "res_lens = defaultdict(int)\n",
    "\n",
    "all_raw_base, all_raw_a, all_raw_b, all_raw_res = json.load(open(data_path, 'r'))\n",
    "\n",
    "print(len(all_raw_base))\n",
    "for raw_res in tqdm(all_raw_res):\n",
    "    \n",
    "    raw_res = ' '.join(raw_res.split())\n",
    "    # 对 res 进行分词\n",
    "    # 利用分词器对各版本代码进行分词\n",
    "    tokens_res = tokenizer.tokenize(raw_res)\n",
    "    ids_res = tokenizer.convert_tokens_to_ids(tokens_res)\n",
    "    # 统计长度\n",
    "    res_lens[len(ids_res) <= 200] += 1\n",
    "\n",
    "print(res_lens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 看看 token 级别 的合并（input_txt） 会不会很诡异"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 conflicts occurred during merge.\n",
      "Merged Tokens:\n",
      "<s>\n",
      "<lbra>\n",
      "Q\n",
      "Scoped\n",
      "Pointer\n",
      "<\n",
      "Q\n",
      "WebSocket\n",
      "Server\n",
      ">\n",
      "Ġm\n",
      "_\n",
      "web\n",
      "Channel\n",
      "Serve\n",
      "er\n",
      ";\n",
      "Ġuint\n",
      "32\n",
      "_\n",
      "t\n",
      "Ġm\n",
      "_\n",
      "web\n",
      "Channel\n",
      "Port\n",
      ";\n",
      "ĠQ\n",
      "Scoped\n",
      "Pointer\n",
      "<\n",
      "Q\n",
      "Web\n",
      "Channel\n",
      ">\n",
      "Ġm\n",
      "_\n",
      "web\n",
      "Channel\n",
      ";\n",
      "Ġbool\n",
      "</s>\n",
      "bool\n",
      "</s>\n",
      "private\n",
      ":\n",
      "ĠV\n",
      "s\n",
      "Quick\n",
      "View\n",
      "<rbra>\n",
      "Ġm\n",
      "_\n",
      "view\n",
      ";\n",
      "ĠV\n",
      "s\n",
      "Server\n",
      "info\n",
      "Ġm\n",
      "_\n",
      "current\n",
      "St\n",
      "udio\n",
      ";\n",
      "ĠQ\n",
      "Scoped\n",
      "Pointer\n",
      "<\n",
      "J\n",
      "ack\n",
      "Trip\n",
      ">\n",
      "Ġm\n",
      "_\n",
      "j\n",
      "ack\n",
      "Trip\n",
      ";\n",
      "ĠQ\n",
      "Shared\n",
      "Pointer\n",
      "<\n",
      "Q\n",
      "J\n",
      "ack\n",
      "Trip\n",
      ">\n",
      "Ġm\n",
      "_\n",
      "standard\n",
      "Window\n",
      ";\n",
      "ĠQ\n",
      "Scoped\n",
      "Pointer\n",
      "<\n",
      "Q\n",
      "Network\n",
      "Access\n",
      "Manager\n",
      ">\n",
      "Ġm\n",
      "_\n",
      "network\n",
      "Access\n",
      "Manager\n",
      ";\n",
      "ĠQ\n",
      "Scoped\n",
      "Pointer\n",
      "<\n",
      "Vs\n",
      "Auth\n",
      ">\n",
      "Ġm\n",
      "_\n",
      "auth\n",
      ";\n",
      "ĠQ\n",
      "Scoped\n",
      "Pointer\n",
      "<\n",
      "Vs\n",
      "Api\n",
      ">\n",
      "Ġm\n",
      "_\n",
      "api\n",
      ";\n",
      "ĠQ\n",
      "Scoped\n",
      "Pointer\n",
      "<\n",
      "Vs\n",
      "Device\n",
      ">\n",
      "Ġm\n",
      "_\n",
      "device\n",
      "Ptr\n",
      ";\n",
      "ĠQ\n",
      "Scoped\n",
      "Pointer\n",
      "<\n",
      "Vs\n",
      "WebSocket\n",
      ">\n",
      "Ġm\n",
      "_\n",
      "st\n",
      "udio\n",
      "Socket\n",
      "Ptr\n",
      ";\n",
      "ĠQ\n",
      "Scoped\n",
      "Pointer\n",
      "<\n",
      "Vs\n",
      "Audio\n",
      ">\n",
      "Ġm\n",
      "_\n",
      "audio\n",
      "Config\n",
      "Ptr\n",
      ";\n",
      "ĠQ\n",
      "Scoped\n",
      "Pointer\n",
      "<\n",
      "Q\n",
      "Thread\n",
      ">\n",
      "Ġm\n",
      "_\n",
      "audio\n",
      "Config\n",
      "Thread\n",
      ";\n",
      "ĠQ\n",
      "Vector\n",
      "<\n",
      "Vs\n",
      "Server\n",
      "Info\n",
      "Pointer\n",
      ">\n",
      "Ġm\n",
      "_\n",
      "servers\n",
      ";\n",
      "ĠQ\n",
      "Vector\n",
      "<\n",
      "Vs\n",
      "Server\n",
      "info\n",
      "*\n",
      ">\n",
      "Ġm\n",
      "_\n",
      "server\n",
      "Model\n",
      ";\n",
      "Ġ//\n",
      "<\n",
      "Ġq\n",
      "ml\n",
      "Ġdoesn\n",
      "'t\n",
      "like\n",
      "Ġsm\n",
      "ĠQ\n",
      "Map\n",
      "<\n",
      "Q\n",
      "String\n",
      ",\n",
      "Ġbool\n",
      ">\n",
      "Ġm\n",
      "_\n",
      "subscribed\n",
      "Servers\n",
      ";\n",
      "ĠQ\n",
      "JsonObject\n",
      "Ġm\n",
      "_\n",
      "regions\n",
      ";\n",
      "ĠQ\n",
      "JsonObject\n",
      "Ġm\n",
      "_\n",
      "user\n",
      "Metadata\n",
      ";\n",
      "ĠQ\n",
      "JsonObject\n",
      "Ġm\n",
      "_\n",
      "network\n",
      "Stats\n",
      ";\n",
      "ĠQ\n",
      "Timer\n",
      "Ġm\n",
      "_\n",
      "start\n",
      "Timer\n",
      ";\n",
      "ĠQ\n",
      "Timer\n",
      "Ġm\n",
      "_\n",
      "refresh\n",
      "Timer\n",
      ";\n",
      "ĠQ\n",
      "Timer\n",
      "Ġm\n",
      "_\n",
      "heartbeat\n",
      "Timer\n",
      ";\n",
      "ĠQ\n",
      "Timer\n",
      "Ġm\n",
      "_\n",
      "network\n",
      "Out\n",
      "age\n",
      "Timer\n",
      ";\n",
      "ĠQ\n",
      "Mutex\n",
      "Ġm\n",
      "_\n",
      "refresh\n",
      "Mutex\n",
      ";\n",
      "ĠQ\n",
      "Url\n",
      "Ġm\n",
      "_\n",
      "st\n",
      "udio\n",
      "To\n",
      "Join\n",
      ";\n",
      "</s>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import tempfile\n",
    "from transformers import RobertaTokenizer\n",
    "\n",
    "# 初始化分词器并添加自定义的特殊 tokens\n",
    "model_type = 'Salesforce/codet5-small'\n",
    "local_path = './codet5/codet5-small'\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(local_path)\n",
    "\n",
    "# 自定义的特殊tokens，用于表示冲突时的括号分隔符\n",
    "brackets_tokens = ['<lbra>', '<mbra>', '<rbra>']\n",
    "succeed_num = tokenizer.add_tokens(brackets_tokens)\n",
    "assert succeed_num == len(brackets_tokens), \"Failed to add all special tokens.\"\n",
    "\n",
    "class ConflictResolver:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.lbra_token = '<lbra>'\n",
    "        self.rbra_token = '<rbra>'\n",
    "\n",
    "    def clean_code(self, code_str):\n",
    "        \"\"\"\n",
    "        清理代码字符串中的多余空格。\n",
    "        \"\"\"\n",
    "        return ' '.join(code_str.split())\n",
    "\n",
    "    def tokenize_code(self, code_str):\n",
    "        \"\"\"\n",
    "        对代码字符串进行分词。\n",
    "        \"\"\"\n",
    "        return self.tokenizer.tokenize(code_str)\n",
    "\n",
    "    def execute_command(self, cmd):\n",
    "        \"\"\"\n",
    "        执行shell命令的辅助函数。\n",
    "        \"\"\"\n",
    "        result = subprocess.run(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "        if result.returncode < 0:\n",
    "            print(\"Error executing command:\", cmd)\n",
    "            print(\"Error message:\", result.stderr)\n",
    "            raise Exception(\"Command failed\")\n",
    "        return result.returncode, result.stdout\n",
    "\n",
    "    def git_merge(self, tokens_base, tokens_a, tokens_b):\n",
    "        \"\"\"\n",
    "        使用git merge-file命令对base、a、b三个版本进行三方合并，\n",
    "        解析git产生的冲突标记并将结果转化为对应的token序列格式。\n",
    "        \"\"\"\n",
    "        with tempfile.TemporaryDirectory() as merge_dir:\n",
    "            base_path = os.path.join(merge_dir, 'base')\n",
    "            a_path = os.path.join(merge_dir, 'a')\n",
    "            b_path = os.path.join(merge_dir, 'b')\n",
    "            merge_output_path = os.path.join(merge_dir, 'merge')\n",
    "\n",
    "            # 将tokens写入临时文件中\n",
    "            with open(base_path, 'w') as f:\n",
    "                f.write('\\n'.join(tokens_base))\n",
    "            with open(a_path, 'w') as f:\n",
    "                f.write('\\n'.join(tokens_a))\n",
    "            with open(b_path, 'w') as f:\n",
    "                f.write('\\n'.join(tokens_b))\n",
    "\n",
    "            # 执行git merge-file命令\n",
    "            merge_cmd = f'git merge-file -L a -L base -L b {a_path} {base_path} {b_path} --diff3 -p > {merge_output_path}'\n",
    "            ret_code, ret_out = self.execute_command(merge_cmd)\n",
    "            if ret_code > 0:\n",
    "                print(\"%s conflicts occurred during merge.\" % ret_code)\n",
    "\n",
    "            # 读取合并结果\n",
    "            with open(merge_output_path, 'r') as f:\n",
    "                merge_res = f.read().splitlines()\n",
    "            merge_res = [x.strip() for x in merge_res if x.strip()]\n",
    "\n",
    "            # 解析冲突标记行\n",
    "            format_ids = [k for k, x in enumerate(merge_res) if x in ['<<<<<<< a', '||||||| base', '=======', '>>>>>>> b']]\n",
    "            if len(format_ids) % 4 != 0:\n",
    "                raise ValueError(\"Unexpected number of conflict markers.\")\n",
    "\n",
    "            final_tokens = []\n",
    "            start = 0\n",
    "            for k in range(0, len(format_ids), 4):\n",
    "                assert (merge_res[format_ids[k]] == '<<<<<<< a' and \n",
    "                        merge_res[format_ids[k + 1]] == '||||||| base' and \n",
    "                        merge_res[format_ids[k + 2]] == '=======' and \n",
    "                        merge_res[format_ids[k + 3]] == '>>>>>>> b'), \"Conflict markers mismatch.\"\n",
    "\n",
    "                # 上下文部分\n",
    "                context_tokens = merge_res[start:format_ids[k]]\n",
    "                # 来自a版本的代码片段\n",
    "                a_tokens = merge_res[format_ids[k] + 1:format_ids[k + 1]]\n",
    "                # base版本的代码片段\n",
    "                base_tokens = merge_res[format_ids[k + 1] + 1:format_ids[k + 2]]\n",
    "                # 来自b版本的代码片段\n",
    "                b_tokens = merge_res[format_ids[k + 2] + 1:format_ids[k + 3]]\n",
    "\n",
    "                start = format_ids[k + 3] + 1\n",
    "\n",
    "                # 添加到最终的token列表中，包含自定义的括号token和sep_token\n",
    "                final_tokens += context_tokens + [self.lbra_token] + a_tokens + [self.tokenizer.sep_token] + base_tokens + [self.tokenizer.sep_token] + b_tokens + [self.rbra_token]\n",
    "\n",
    "            # 处理剩余的尾部内容\n",
    "            if start < len(merge_res):\n",
    "                final_tokens += merge_res[start:]\n",
    "\n",
    "            # 在最终序列的首尾加入bos和eos\n",
    "            final_tokens = [self.tokenizer.bos_token] + final_tokens + [self.tokenizer.eos_token]\n",
    "\n",
    "            return final_tokens\n",
    "\n",
    "    def resolve_conflict(self, base, a, b):\n",
    "        \"\"\"\n",
    "        主函数：处理输入的base、a、b代码并返回合并后的token序列。\n",
    "        \"\"\"\n",
    "        # 清理代码\n",
    "        base_clean = self.clean_code(base)\n",
    "        a_clean = self.clean_code(a)\n",
    "        b_clean = self.clean_code(b)\n",
    "\n",
    "        # 分词\n",
    "        tokens_base = self.tokenize_code(base_clean)\n",
    "        tokens_a = self.tokenize_code(a_clean)\n",
    "        tokens_b = self.tokenize_code(b_clean)\n",
    "\n",
    "        # 执行合并\n",
    "        merged_tokens = self.git_merge(tokens_base, tokens_a, tokens_b)\n",
    "\n",
    "        return merged_tokens\n",
    "\n",
    "def main():\n",
    "    # 示例代码输入\n",
    "    # base_code = \"\"\"\n",
    "    #     log_denominator_n = logsumexp(jf_k - ju_kn.T, b=jNN_k, axis=1)\n",
    "    #     log_numerator_k_= logsumexp(-log_denominator_n - ju_kn, axis=1)\n",
    "    #     return -1 * jN_k * (1.0 - npj.exp(jf_k + log_numerator_k))\n",
    "    # jit_mbar_gradient = jax.jit(jax_mbar_gradient)\n",
    "    # \"\"\"\n",
    "\n",
    "    # a_code = \"\"\"\n",
    "    #     return -1 * jNk * (1.0 - jnp.exp(f_k + log_numerateor_k)\n",
    "    # jit_mbar_gradient= jax.jit(jax_mbar_gradient)\n",
    "    # \"\"\"\n",
    "\n",
    "    # b_code = \"\"\"\n",
    "    #     return -1 * jNk * (1.0 - jpn.exp(f_k + log_numerator_k))\n",
    "    # \"\"\"\n",
    "    \n",
    "    base_code = \"\"\"\n",
    "        bool m_showFirstRun = false;\n",
    "        bool m_checkssl     = true;\n",
    "        bool m_vsModeActive = false;\n",
    "    \"\"\"\n",
    "\n",
    "    a_code = \"\"\"\n",
    "        QScopedPointer<QWebSocketServer> m_webChannelServeer;\n",
    "        uint32_t m_webChannelPort;\n",
    "        QScopedPointer<QWebChannel> m_webChannel;\n",
    "        bool m_showFirstRun = false;\n",
    "        bool m_checkssl\n",
    "        = true;\n",
    "        bool m_vsModeActive = false;\n",
    "    \"\"\"\n",
    "\n",
    "    b_code = \"\"\"\n",
    "    private:\n",
    "        VsQuickView m_view;\n",
    "        VsServerinfo m_currentStudio;\n",
    "        QScopedPointer<JackTrip> m_jackTrip;\n",
    "        QSharedPointer<QJackTrip> m_standardWindow;\n",
    "        QScopedPointer<QNetworkAccessManager> m_networkAccessManager;\n",
    "        QScopedPointer<VsAuth> m_auth;\n",
    "        QScopedPointer<VsApi> m_api;\n",
    "        QScopedPointer<VsDevice> m_devicePtr;\n",
    "        QScopedPointer<VsWebSocket> m_studioSocketPtr;\n",
    "        QScopedPointer<VsAudio> m_audioConfigPtr;\n",
    "        QScopedPointer<QThread> m_audioConfigThread;\n",
    "        QVector<VsServerInfoPointer> m_servers;\n",
    "        QVector<VsServerinfo*> m_serverModel; //< qml doesn'tlike sm\n",
    "        QMap<QString, bool> m_subscribedServers;\n",
    "        QJsonObject m_regions;\n",
    "        QJsonObject m_userMetadata;\n",
    "        QJsonObject m_networkStats;\n",
    "        QTimer m_startTimer;\n",
    "        QTimer m_refreshTimer;\n",
    "        QTimer m_heartbeatTimer;\n",
    "        QTimer m_networkOutageTimer;\n",
    "        QMutex m_refreshMutex;\n",
    "        QUrl m_studioToJoin;\n",
    "    \"\"\"\n",
    "\n",
    "    resolver = ConflictResolver(tokenizer)\n",
    "    merged_tokens = resolver.resolve_conflict(base_code, a_code, b_code)\n",
    "\n",
    "    # 打印每个token一行\n",
    "    print(\"Merged Tokens:\")\n",
    "    for token in merged_tokens:\n",
    "        print(token)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1011176/900714252.py:55: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=args.device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型加载完成！\n",
      "Resolved Code:\n",
      "bool m_showFirstRun = false; bool m2checkssl = true; QScopedPointer<QWebSocketServer> m _webChannelServeer; uint32_t m_%sLastRunTime; VsServerinfo mCurrentEntryPoint; SvgData m0; switch (m_currentStudio) { case VSvgView: V s_view.getViewPointByChannel; break; case VK_USER_DO_NOT_USE_OR_YOU_WILL_BE_FIRED: return; } } /** * This method is neccessary to join a stream. * * @param stream the stream to start from. */ public static QStream Stream(VsQuickView stream) : base(stream); } public void start() { super.start(); end(); } @Override protected void onJoin(){ super._onJoin(); long startTime = 0; long lastFrameSize = System.currentTimeMillis(); V\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import RobertaTokenizer, T5ForConditionalGeneration\n",
    "from collections import namedtuple\n",
    "\n",
    "# 定义 dotdict 类，允许通过属性访问字典内容\n",
    "class dotdict(dict):\n",
    "    def __getattr__(self, name):\n",
    "        return self[name]\n",
    "\n",
    "# 定义 MergeT5 类，继承自 nn.Module\n",
    "class MergeT5(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(MergeT5, self).__init__()\n",
    "        # 使用预训练的 T5ForConditionalGeneration 模型\n",
    "        self.t5 = T5ForConditionalGeneration.from_pretrained(args.model_type)\n",
    "        # 调整词表大小，包含新增的自定义 tokens\n",
    "        self.t5.resize_token_embeddings(len(args.tokenizer))\n",
    "        self.embedding_dim = self.t5.config.hidden_size\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, decoder_input_ids=None, **kwargs):\n",
    "        return self.t5(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "# 配置参数\n",
    "args = dotdict({\n",
    "    'model_type': './codet5/codet5-small',  # 模型路径\n",
    "    'max_conflict_length': 500,\n",
    "    'max_resolve_length': 200,\n",
    "    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "})\n",
    "\n",
    "# 初始化分词器并添加自定义的特殊 tokens\n",
    "tokenizer = RobertaTokenizer.from_pretrained(args.model_type)\n",
    "brackets_tokens = ['<lbra>', '<mbra>', '<rbra>']\n",
    "succeed_num = tokenizer.add_tokens(brackets_tokens)\n",
    "assert succeed_num == len(brackets_tokens), \"Failed to add all special tokens.\"\n",
    "args.tokenizer = tokenizer  # 将 tokenizer 添加到 args 中\n",
    "\n",
    "# 初始化模型\n",
    "model = MergeT5(args)\n",
    "model.to(args.device)\n",
    "\n",
    "# 加载训练好的模型权重\n",
    "model_path = 'back/best_model.pt'  # 确保路径正确\n",
    "\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    raise FileNotFoundError(f\"Model file not found at {model_path}\")\n",
    "model.load_state_dict(torch.load(model_path, map_location=args.device))\n",
    "model.eval()\n",
    "print(\"模型加载完成！\")\n",
    "\n",
    "\n",
    "# 定义冲突解决函数\n",
    "def resolve_conflict(model, tokenizer, base_code, a_code, b_code, args, beam_num=3):\n",
    "    \"\"\"\n",
    "    生成代码合并冲突的解决方案。\n",
    "\n",
    "    参数:\n",
    "    - model: 已加载的 MergeT5 模型\n",
    "    - tokenizer: 分词器\n",
    "    - base_code: base 分支的代码字符串\n",
    "    - a_code: a 分支的代码字符串\n",
    "    - b_code: b 分支的代码字符串\n",
    "    - args: 配置参数\n",
    "    - beam_num: beam search 的宽度\n",
    "\n",
    "    返回:\n",
    "    - resolved_code: 解决冲突后的代码字符串\n",
    "    \"\"\"\n",
    "    # 清理代码（移除多余空格）\n",
    "    def clean_code(code_str):\n",
    "        return ' '.join(code_str.split())\n",
    "\n",
    "    base_clean = clean_code(base_code)\n",
    "    a_clean = clean_code(a_code)\n",
    "    b_clean = clean_code(b_code)\n",
    "\n",
    "    # 分词\n",
    "    tokens_base = tokenizer.tokenize(base_clean)\n",
    "    tokens_a = tokenizer.tokenize(a_clean)\n",
    "    tokens_b = tokenizer.tokenize(b_clean)\n",
    "\n",
    "    # 构造合并后的 token 序列，包含特殊的括号 tokens\n",
    "    # 格式：<s> context <lbra> a_code <sep> base_code <sep> b_code <rbra> </s>\n",
    "    merged_tokens = [tokenizer.bos_token] + tokens_base + [tokenizer.sep_token, '<lbra>'] + tokens_a + [tokenizer.sep_token] + tokens_b + [tokenizer.sep_token, '<rbra>', tokenizer.eos_token]\n",
    "    \n",
    "    # 转换为 token ids\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(merged_tokens)\n",
    "    input_ids = torch.tensor([input_ids]).to(args.device)\n",
    "\n",
    "    # 创建 attention mask\n",
    "    attention_mask = (input_ids != tokenizer.pad_token_id).long()\n",
    "\n",
    "    # 生成输出\n",
    "    with torch.no_grad():\n",
    "        outputs = model.t5.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            num_beams=beam_num,\n",
    "            max_length=args.max_resolve_length,\n",
    "            early_stopping=True,\n",
    "            no_repeat_ngram_size=2\n",
    "        )\n",
    "\n",
    "    # 解码输出\n",
    "    resolved_code = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return resolved_code\n",
    "\n",
    "# 示例代码输入\n",
    "base_code = \"\"\"\n",
    "    bool m_showFirstRun = false;\n",
    "    bool m_checkssl     = true;\n",
    "    bool m_vsModeActive = false;\n",
    "\"\"\"\n",
    "\n",
    "a_code = \"\"\"\n",
    "    QScopedPointer<QWebSocketServer> m_webChannelServeer;\n",
    "    uint32_t m_webChannelPort;\n",
    "    QScopedPointer<QWebChannel> m_webChannel;\n",
    "    bool m_showFirstRun = false;\n",
    "    bool m_checkssl\n",
    "    = true;\n",
    "    bool m_vsModeActive = false;\n",
    "\"\"\"\n",
    "\n",
    "b_code = \"\"\"\n",
    "private:\n",
    "    VsQuickView m_view;\n",
    "    VsServerinfo m_currentStudio;\n",
    "    QScopedPointer<JackTrip> m_jackTrip;\n",
    "    QSharedPointer<QJackTrip> m_standardWindow;\n",
    "    QScopedPointer<QNetworkAccessManager> m_networkAccessManager;\n",
    "    QScopedPointer<VsAuth> m_auth;\n",
    "    QScopedPointer<VsApi> m_api;\n",
    "    QScopedPointer<VsDevice> m_devicePtr;\n",
    "    QScopedPointer<VsWebSocket> m_studioSocketPtr;\n",
    "    QScopedPointer<VsAudio> m_audioConfigPtr;\n",
    "    QScopedPointer<QThread> m_audioConfigThread;\n",
    "    QVector<VsServerInfoPointer> m_servers;\n",
    "    QVector<VsServerinfo*> m_serverModel; //< qml doesn'tlike sm\n",
    "    QMap<QString, bool> m_subscribedServers;\n",
    "    QJsonObject m_regions;\n",
    "    QJsonObject m_userMetadata;\n",
    "    QJsonObject m_networkStats;\n",
    "    QTimer m_startTimer;\n",
    "    QTimer m_refreshTimer;\n",
    "    QTimer m_heartbeatTimer;\n",
    "    QTimer m_networkOutageTimer;\n",
    "    QMutex m_refreshMutex;\n",
    "    QUrl m_studioToJoin;\n",
    "\"\"\"\n",
    "\n",
    "# 生成解决方案\n",
    "resolved_code = resolve_conflict(model, tokenizer, base_code, a_code, b_code, args, beam_num=3)\n",
    "\n",
    "# 输出结果\n",
    "print(\"Resolved Code:\")\n",
    "print(resolved_code)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ase-merge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
