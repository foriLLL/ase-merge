{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意到 `dataset.py` 中直接从 json 中读取了数据，但数据集并不符合这种格式，所以需要将数据集转换为 json 格式。\n",
    "\n",
    "```py\n",
    "# 复现代码中读取数据集的部分\n",
    "            all_raw_base, all_raw_a, all_raw_b, all_raw_res = json.load(open('%s/raw_data'%(total_raw_data_path)))\n",
    "```\n",
    "\n",
    "所以我们需要将数据集转换为 json 格式：\n",
    "\n",
    "```\n",
    "[\n",
    "    [\"base1\", \"base2\", \"base3\"],\n",
    "    [\"a1\", \"a2\", \"a3\"],\n",
    "    [\"b1\", \"b2\", \"b3\"],\n",
    "    [\"res1\", \"res2\", \"res3\"]\n",
    "]\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48785/48785 [00:18<00:00, 2667.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151426\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'RAW_DATA/fse2022'\n",
    "out_file = 'RAW_DATA/raw_data'\n",
    "# walk 递归找到目录下的所有 json 文件\n",
    "import os\n",
    "\n",
    "def get_all_json_files(path):\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            if file.endswith('metadata.json'):\n",
    "                yield os.path.join(root, file)\n",
    "\n",
    "all_json_files = list(get_all_json_files(data_dir))\n",
    "print(len(all_json_files))\n",
    "\n",
    "# 读取 json 文件\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "o_contents = []\n",
    "a_contents = []\n",
    "b_contents = []\n",
    "r_contents = []\n",
    "for file in tqdm(all_json_files):\n",
    "    with open(file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        for chunk in data['conflicting_chunks']:\n",
    "            if chunk['res_region'] is None:\n",
    "                continue\n",
    "            o_contents.append(chunk['base_contents'])\n",
    "            a_contents.append(chunk['a_contents'])\n",
    "            b_contents.append(chunk['b_contents'])\n",
    "            r_contents.append(chunk['res_region'])\n",
    "    \n",
    "assert len(o_contents) == len(a_contents) == len(b_contents) == len(r_contents)\n",
    "print(len(o_contents))\n",
    "\n",
    "json_arr = [\n",
    "    o_contents,\n",
    "    a_contents,\n",
    "    b_contents,\n",
    "    r_contents\n",
    "]\n",
    "\n",
    "# 把 json_arr 写入文件\n",
    "with open(out_file, 'w') as f:\n",
    "    json.dump(json_arr, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自己收集的数据集 .json 转化为 raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "358446\n",
      "358446\n",
      "358446\n",
      "358446\n"
     ]
    }
   ],
   "source": [
    "data_dir = '/root/projects/conflictManager/edit_script_resolver/train_and_infer/data/processed_data/recollect_without_min_bundle_without_file_content'\n",
    "out_file = 'RAW_DATA/graphQL_raw_data_sample_20'\n",
    "\n",
    "\n",
    "# 1. 列出 data_dir 下所有 xx.json 文件\n",
    "def get_all_json_files(path):\n",
    "    import os\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            if file.endswith('.json'):\n",
    "                from pathlib import Path\n",
    "                basename = Path(file).stem\n",
    "                idx = basename.split('.')[0]\n",
    "                yield (os.path.join(root, file), idx)\n",
    "\n",
    "tuples = tuple(get_all_json_files(data_dir))\n",
    "\n",
    "o_contents = []\n",
    "a_contents = []\n",
    "b_contents = []\n",
    "r_contents = []\n",
    "\n",
    "\n",
    "# 2. 读取 json 文件\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "for file, idx in tqdm(tuples, dynamic_ncols=True, desc='Reading json files', leave=False, position=0):\n",
    "    if (int(idx) >= 20): continue\n",
    "    with open(file, 'r') as f:\n",
    "        cfs = json.load(f)\n",
    "        for cf in tqdm(cfs, dynamic_ncols=True, desc='Reading conflict chunks', leave=False, position=1):\n",
    "            for chunk in cf['conflict_chunks']:\n",
    "                o_contents.append(chunk['o_content'])\n",
    "                a_contents.append(chunk['a_content'])\n",
    "                b_contents.append(chunk['b_content'])\n",
    "                r_contents.append(chunk['r_content'])\n",
    "    assert len(o_contents) == len(a_contents) == len(b_contents) == len(r_contents)\n",
    "\n",
    "print(len(o_contents))\n",
    "print(len(a_contents))\n",
    "print(len(b_contents))\n",
    "print(len(r_contents))\n",
    "\n",
    "json_arr = [\n",
    "    o_contents,\n",
    "    a_contents,\n",
    "    b_contents,\n",
    "    r_contents\n",
    "]\n",
    "\n",
    "# 把 json_arr 写入文件\n",
    "with open(out_file, 'w') as f:\n",
    "    json.dump(json_arr, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 看看 token_len 分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 151426/151426 [04:03<00:00, 620.89it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {True: 138641, False: 12785})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 找到所有符合这个模式的文件\n",
    "\n",
    "# data_path = 'RAW_DATA/raw_data'\n",
    "data_path = 'RAW_DATA/graphQL_raw_data_sample_20'\n",
    "\n",
    "# 内容是 all_raw_base, all_raw_a, all_raw_b, all_raw_res = json.load(open(data_path, 'r'))\n",
    "# 统计所有 inputs 和 outputs 的长度分布\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "from tqdm import tqdm   \n",
    "from collections import defaultdict\n",
    "from transformers import RobertaTokenizer, T5Model, T5ForConditionalGeneration, AdamW\n",
    "\n",
    "# 模型类型设定为 CodeT5 的小模型\n",
    "model_type = 'Salesforce/codet5-small'\n",
    "local_path = './codet5/codet5-small'\n",
    "\n",
    "# 初始化对应的分词器\n",
    "# tokenizer = RobertaTokenizer.from_pretrained(model_type)\n",
    "tokenizer = RobertaTokenizer.from_pretrained(local_path)\n",
    "\n",
    "\n",
    "# inputs_lens = defaultdict(int)\n",
    "# outputs_lens = defaultdict(int)\n",
    "res_lens = defaultdict(int)\n",
    "\n",
    "all_raw_base, all_raw_a, all_raw_b, all_raw_res = json.load(open(data_path, 'r'))\n",
    "\n",
    "print(len(all_raw_base))\n",
    "for raw_res in tqdm(all_raw_res):\n",
    "    \n",
    "    raw_res = ' '.join(raw_res.split())\n",
    "    # 对 res 进行分词\n",
    "    # 利用分词器对各版本代码进行分词\n",
    "    tokens_res = tokenizer.tokenize(raw_res)\n",
    "    ids_res = tokenizer.convert_tokens_to_ids(tokens_res)\n",
    "    # 统计长度\n",
    "    res_lens[len(ids_res) <= 200] += 1\n",
    "\n",
    "print(res_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ase-merge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
